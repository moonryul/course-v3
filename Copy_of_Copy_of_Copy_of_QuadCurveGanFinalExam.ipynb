{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of QuadCurveGanFinalExam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNusFEZrM/alxaxh1SQbVg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonryul/course-v3/blob/master/Copy_of_Copy_of_Copy_of_QuadCurveGanFinalExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMADk4rjxJ3e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1EU2hJoxQYJ"
      },
      "source": [
        "Data & AI Final Exam. The final exam is focused on Generative Adversarial Networks (GANs).\r\n",
        "https://wiki.pathmind.com/generative-adversarial-network-gan\r\n",
        "\r\n",
        " They are deep neural net architectures composed of a pair of competing neural networks: a generator and a discriminator. This model is trained by alternately optimizing two objective functions so that the generator G learns to produce samples resembling real images, and the discriminator D learns to better discriminate between real and fake data.\r\n",
        "\r\n",
        "GAN is important because it has a huge potential. So, we want to understand its principle well enough. \r\n",
        "\r\n",
        "Please read the following gan code and comments carefully and write down answers to the questions in this notebook. Submit the notebook to your github repo. This code is executable on colab as it is. You had better answer the questions starting from Question 1. The order of questions is not sequential from top to bottom. Notice that answers to many questions can be obtained when you read the code and the comments carefully. The answer to a question may be found in another question itself. But all these will be seen only to those who understand what is going on. ** Please do not copy words of comments when you write down your answers. Write your own words** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-SdgbgrZH0E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LvZrYGeU8-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72551d90-8db0-4d66-9cad-dcef307f119a"
      },
      "source": [
        "!pip install tensorflow\r\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (50.3.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.19.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqun5vTwVH2s"
      },
      "source": [
        "# train a generative adversarial network on a one-dimensional function\r\n",
        "from numpy import hstack\r\n",
        "from numpy import zeros\r\n",
        "from numpy import ones\r\n",
        "from numpy.random import rand\r\n",
        "from numpy.random import randn\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from matplotlib import pyplot\r\n",
        "\r\n",
        "\r\n",
        "# define the standalone discriminator model\r\n",
        "def define_discriminator(n_inputs=2):\r\n",
        "    # class Sequential(Model) = Linear stack of layers\r\n",
        "    # The `Model` class adds training & evaluation routines to a `Network`.\r\n",
        "    # class Model(Network): add(self, layer): \tAdds a layer instance on top of the layer stack.\r\n",
        "\r\n",
        "    model = Sequential()  # model is an object of class Sequential\r\n",
        "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\r\n",
        "    # n_inputs = 2; n_output=25:  The Input vector is a 2D point (x,y)\r\n",
        "    model.add(Dense(1, activation='sigmoid'))\r\n",
        "    # n_input = 25 = n_output of the previous layer; n_output =1 ( The value of the discriminator output is probability between  o and 1\r\n",
        "    # compile model\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "    # model.loss = loss =\"binary_crossentropy\": Using the binary cross entropy function as the loss\r\n",
        "    # function means that it tries to minimize the difference (loss) between the predicted probability of the net\r\n",
        "    # for the input to belong to one of the two categories and the labeled probability which is either 1 (real)or 0 (fake).\r\n",
        "    # It tries to minimize this difference (loss) for all inputs (whose number is 128 in our example); it means that\r\n",
        "    # it tries ot minimize the average difference (loss) of the all inputs. The basic idea is the same with the mean square error\r\n",
        "    # loss for regression problem. The difference is the \"value\" used to compute the loss probability rather than ordinary values.\r\n",
        "\r\n",
        "    return model  # model is a reference  to the current instance of the class\r\n",
        "\r\n",
        "\r\n",
        "# define the standalone generator model\r\n",
        "def define_generator(latent_dim, n_outputs=2):  # latent_dim =5\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\r\n",
        "                    input_dim=latent_dim))  # n_input=5 = latent_dim; n_output=15\r\n",
        "    model.add(Dense(n_outputs, activation='linear'))  # n_input = 15 = n_output of the previous layer; n_output = 2\r\n",
        "    # The dimension of the output of the geneator is 2, \r\n",
        "    # because it generates a 2D point (x,y) which is supposed to lie on\r\n",
        "    # the quadratic curve y = x^2;\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# define the combined generator and discriminator model for updating the generator\r\n",
        "def define_gan(generator, discriminator):\r\n",
        "    # make weights in the discriminator not trainable\r\n",
        "    discriminator.trainable = False  # discriminator is set as not trainable when it is part of the composite model\r\n",
        "    # But it is trainable when it is used alone\r\n",
        "    # connect them\r\n",
        "    model = Sequential()\r\n",
        "    # add generator\r\n",
        "    model.add(generator)\r\n",
        "    # add the discriminator\r\n",
        "    model.add(discriminator)\r\n",
        "    # compile model\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\r\n",
        "    # model.loss = loss =\"binary_crossentropy\"\r\n",
        "    return model\r\n",
        "\r\n",
        "#  Making the discriminator not trainable is a clever trick in the Keras API. The trainable\r\n",
        "#  property impacts the model when it is compiled. The discriminator model was compiled with\r\n",
        "#  trainable layers, therefore the model weights in those layers will be updated when the standalone\r\n",
        "#  model is updated via calls to train on batch().\r\n",
        "\r\n",
        "# generate n real samples with class labels \"ones\" for training the discriminator\r\n",
        "def generate_real_samples(n): # n = 128/2\r\n",
        "    # generate inputs in [-0.5, 0.5]\r\n",
        "    X1 = rand(n) - 0.5\r\n",
        "    # generate outputs X^2\r\n",
        "    X2 = X1 * X1\r\n",
        "    # stack arrays\r\n",
        "    X1 = X1.reshape(n, 1)\r\n",
        "    X2 = X2.reshape(n, 1)\r\n",
        "    X = hstack((X1, X2))  # X =  hstack( [1,2], [3,4] ) ==>[ [1,3],[2,4] ] : 128 points\r\n",
        "    # generate class labels\r\n",
        "    y = ones((n, 1))  # y = 128/2 labels\r\n",
        "    return X, y  # # A pair of 128/2 real samples and their 128 labels\r\n",
        "\r\n",
        "\r\n",
        "# generate points in latent space as input for the generator\r\n",
        "def generate_latent_points(latent_dim, n):\r\n",
        "    # generate points in the latent space\r\n",
        "    x_input = randn(latent_dim * n)  # [01, 02, 0.9,...., 0,1]\r\n",
        "    # reshape into a batch of inputs for the network\r\n",
        "    x_input = x_input.reshape(n, latent_dim)  # 128 * 5 matrix\r\n",
        "    return x_input\r\n",
        "\r\n",
        "\r\n",
        "# use the generator to generate n fake examples, with class labels \"zero\", for training the discriminator\r\n",
        "def generate_fake_samples(generator, latent_dim, n): # n = 128/2\r\n",
        "    # generate points in latent space\r\n",
        "    x_input = generate_latent_points(latent_dim, n)  # 128/2  x 5: 128/2 samples of 5 random numbers\r\n",
        "    # predict outputs\r\n",
        "    X = generator.predict(x_input)  # X = 128/2 generator outputs for 128/2 samples of 5 numbers\r\n",
        "    # create class labels\r\n",
        "    y = zeros((n, 1))  # y = 128/2  labels\r\n",
        "    return X, y  # A pair of 128/2 fake samples and their 128/2 labels\r\n",
        "\r\n",
        "\r\n",
        "# evaluate the discriminator and plot real and fake points\r\n",
        "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\r\n",
        "    # prepare real samples\r\n",
        "    x_real, y_real = generate_real_samples(n)  # (x_real, y_real):  A pair of 128 real samples and their 128 labels\r\n",
        "    # evaluate discriminator on real examples, that is, compute the accuracy function of the net\r\n",
        "    # The output of the discriminator net is the probability that an input has with respect to being fake or real.\r\n",
        "    # If the probability is near 1, it is likely that the input is real, and vice versa.\r\n",
        "    # If the proability is 0.5, it means that the discriminator cannot tell if the input is real or fake.\r\n",
        "    # This state is the best that the generator tries to achieve; the generator does not want to be too much \r\n",
        "    # similar to real data. It wants to produce novel data, but it does not want to be refused by the \r\n",
        "    # discriminator or critic. \r\n",
        "     \r\n",
        "    _, acc_real = discriminator.evaluate(x_real, y_real,\r\n",
        "                                         verbose=0)  # acc_real = THe accuray of the discriminator net that tells \"real\" for real samples (inputs)\r\n",
        "    # prepare fake examples\r\n",
        "    x_fake, y_fake = generate_fake_samples(generator, latent_dim,\r\n",
        "                                           n)  # (x_fake, y_fake):  # A pair of 128 fake samples and their 128 labels\r\n",
        "    # evaluate discriminator on fake examples\r\n",
        "    _, acc_fake = discriminator.evaluate(x_fake, y_fake,\r\n",
        "                                         verbose=0)  # acc_fake = The accuray of the discriminator net that tells \"fake\" for fake samples (inputs)\r\n",
        "    # summarize discriminator performance\r\n",
        "    print(epoch, acc_real, acc_fake)  # print both acc_real and acc_fake for the current epoch.\r\n",
        "    # scatter plot real and fake data points: \r\n",
        "    # x_real[i,0] is the x coord of ith real point and x_real[i,1] is the y coord of ith real point\r\n",
        "\r\n",
        "    pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\r\n",
        "    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\r\n",
        "    # save plot to file\r\n",
        "    filename = 'generated_plot_e%03d.png' % (epoch + 1)\r\n",
        "    pyplot.savefig(filename)\r\n",
        "    pyplot.close()\r\n",
        "\r\n",
        "\r\n",
        "# train the generator and discriminator\r\n",
        "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\r\n",
        "    # determine half the size of one batch, for updating the discriminator\r\n",
        "    half_batch = int(n_batch / 2)\r\n",
        "\r\n",
        "    # loop over epochs: In the case of typical supervised learning, one epoch refers to one scan over the entire dataset\r\n",
        "    # in the process of training the network.\r\n",
        "    # In this gan example, one epoch refers to one scan over a single mini-batch.\r\n",
        "    # This is because the real data is prepared not in the form of entire dataset in the beginning,\r\n",
        "    # but is computed in the form of mini-batch by \"generate_real_samples\" function at each update of the discriminator.\r\n",
        "    #\r\n",
        "    for i in range(n_epochs):\r\n",
        "        # 1) Train the discriminator to discriminate between real-data and fake-data\r\n",
        "\r\n",
        "        # 1.1)  prepare real samples (real mini-batch): note that in this example, the discriminator will see\r\n",
        "        #  10000 * 128/2 real data ( 2D points from the quadratic curve y = x^2) in total throughout all the epochs.\r\n",
        "        # It means also that the discriminator encounters the same number of fake 2D points throughout all the epochs.\r\n",
        "        # and is trained to tell \"fake\" to them. The fake data used by the discriminator become smarter as the epoch\r\n",
        "        # progresses.\r\n",
        "\r\n",
        "        x_real, y_real = generate_real_samples(half_batch)\r\n",
        "\r\n",
        "# Q3. In each epoch,  x_real and y_real are created.\r\n",
        "#  Q3.1) What are these?\r\n",
        "#  Q3.2) In this code, how many real samples are created in each epoch? \r\n",
        "\r\n",
        "\r\n",
        "        # prepare fake examples (fake mini_batch)\r\n",
        "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\r\n",
        "\r\n",
        "# Q4. In each epoch, x_fake and y_fake are generated. They are generated by using \"g_model\".\r\n",
        "# Q4.1) What are they? \r\n",
        "# Q4.2) How many \"faked\" data are generated? \r\n",
        "# Q4.3) Initially the generator \"g_model\" would be very poor at generating\r\n",
        "#        reasonable data. What is the use to these poor data?\r\n",
        "\r\n",
        "\r\n",
        "        # 1.2)  update discriminator\r\n",
        "        # \"Runs a single gradient update on a single mini-batch of data\":\r\n",
        "        d_model.train_on_batch(x_real, y_real)  # train the discriminator using the real mini-batch\r\n",
        "\r\n",
        "# Q5. The discriminator \"d_model\" is trained by using x_real, y_real. \r\n",
        "#  Q5.1) What value does \"y_real\" have?\r\n",
        "#  Q5.2) What is the goal of this training?\r\n",
        "# That is, what is the network trained to achieve by using these data?\r\n",
        "\r\n",
        "        d_model.train_on_batch(x_fake,y_fake)  # train the discriminator using the fake mini-batch.\r\n",
        "\r\n",
        "# Q6. The discriminator \"d_model\" is also trained by using x_fake, y_fake.\r\n",
        "# Q6.1) What value does \"y_fake\" have?\r\n",
        "# Q6.2) What is the goal of this training? That is, what is the network trained to do by using these data?\r\n",
        "\r\n",
        "# Q7. Why do you train \"d_model\" on both x_real, y_real and x_fake, y_fake?\r\n",
        "\r\n",
        "\r\n",
        "# Q8. Now in the current epoch, after the discriminator is trained both on real data and fake data, the\r\n",
        "#     generator is trained as follows:\r\n",
        "\r\n",
        "# Q8.1) What is the goal of the training of the generator? That is, what is the net trained to achieve?\r\n",
        "\r\n",
        "        # 2) Train the generator net\r\n",
        "        # 2.1) prepare points in latent space as input for the generator\r\n",
        "        x_gan = generate_latent_points(latent_dim,\r\n",
        "                                       n_batch)  # x_gan = 128 x 5 matrix; This means 128 samples of 5 random numbers\r\n",
        "        # 2.2) create labels that the discriminator should produce for the fake sample output of the generator\r\n",
        "        # Note that while the generator is updated in order to \"fool\" the discriminator, the discriminator is fixed.\r\n",
        "        y_gan = ones((n_batch, 1))  # 128 1's\r\n",
        "\r\n",
        "        # 2.3) update the generator  so that the loss of the fixed discriminator is minimized.\r\n",
        "        gan_model.train_on_batch(x_gan, y_gan)   # x_gan= input to the generator = 128 samples of 5 random vectors,\r\n",
        "                                                 # y_gan = target label = 128 labels (all 1's) for the discriminator\r\n",
        "\r\n",
        "# Q8.2)  What is the size of x_gan and y_gan?\r\n",
        "# Q8.3) Although \"gan_model.train_on_batch()\" may imply that gan_model is trained, \r\n",
        "#       in fact, x_gan and y_gan are  used to train the generator part of gan_model.\r\n",
        "#      But then why is gan_model referred to (which contains the discriminator net) to train the generator?\r\n",
        "\r\n",
        "# Q8.4) Is the discriminator in gan_model updated (trained) when the generator part is updated (trained)?\r\n",
        "#        Explain why.\r\n",
        "\r\n",
        "\r\n",
        "        # NOTE: gan_model.train_on_batch(x_gan, y_gan) tries to train  the generator so that the discriminator tells \"real\"\r\n",
        "        # for EVERY (fake) output generated by the generator. Note that this goal is a bit indirect in the sense that\r\n",
        "        # the generator does not attempt to optimize its output directly but it tries to optimize the\r\n",
        "        # the output of the discriminator. But it is not uncommon. For example, consider the effort of parents\r\n",
        "        # with respect to their children.\r\n",
        "\r\n",
        "        # evaluate the model every n_eval epochs\r\n",
        "        if (i + 1) % n_eval == 0:\r\n",
        "            summarize_performance(i, g_model, d_model, latent_dim)\r\n",
        "            \r\n",
        "#Q9. Q9.1) What does the above statement do?\r\n",
        "#    Q9.2) Read the result of the above statement. Explain what happens as the epoch progresses.\r\n",
        "#    Q9.3) Do you think that the gan would happy if the real-accuray and the fake-accuracy are similar?\r\n",
        "#          If so, explain why.\r\n",
        "\r\n",
        "#    Q9.4) In the content folder on the left column, you can see image files that show the progress of\r\n",
        "#          the generator; It describes the report of the performance in a different way. \r\n",
        "#          Everything may look as you expected or not. \r\n",
        "\r\n",
        "#          Copy the images showing unexpected results to the notebook and explain\r\n",
        "#          what are the unexpected results. You can also refer to the report of the performance \r\n",
        "#          in your  explantion.  If no picture shows unexpected results, then explain why you think so,\r\n",
        "#          referring the pictures and the report of the performance.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0d0_HSpI7YL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzTwGhPVI7vM"
      },
      "source": [
        "Questions: In the code above, functions need to build and train the gan are defined.\r\n",
        "In the code below, these functions are used to actually train the gan. This gan\r\n",
        "is a network that generates points on quadratic curve y = x^2, \r\n",
        "                                              where x is in [-0.5,0.5]. \r\n",
        "The gan does not know that these points come form the quadratic curve.\r\n",
        "This network can be constructed and trained by means of supervised learning \r\n",
        "technique. That would be better for this problem. But in order to understand what\r\n",
        "is going on in gan, we will use this simple toy problem. \r\n",
        "\r\n",
        "Q1. Read function \"define_generator\". \r\n",
        "This function does not return some numbers, but some\r\n",
        "abstract entity.\r\n",
        "\r\n",
        "Q1.1) What kind of entity is it? \r\n",
        "It is something that transforms an input to an output. \r\n",
        "\r\n",
        "Q1.2) What is its input? \r\n",
        "Q1.3) What is its output?\r\n",
        "Q1.4) When the generator is first created, what values \r\n",
        "would its parameters/weights have? The values of the parameters will be updated as\r\n",
        "the training is going on. But what values would they have initially? \r\n",
        "\r\n",
        "Q2. Read function \"define_discriminator\". This function defines a discriminator \r\n",
        "network which classifies its input as \"fake\" (0) or as \"real\" (1).\r\n",
        "Q2.1) What is the dimension of the input vector? \r\n",
        "Q2.2) Why does the dimension of the input vector have that particular number?\r\n",
        "Q2.3) What is the dimension of the output?\r\n",
        "Q2.4) Why is it so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY9MVP2yVZAd"
      },
      "source": [
        "# size of the latent space (the size of a random input vector to the generator)\r\n",
        "latent_dim = 5\r\n",
        "# create the discriminator\r\n",
        "discriminator = define_discriminator()\r\n",
        "#  discriminator is a reference  to the instance of the Sequential class\r\n",
        "#  discriminator defines the loss function and the optimization method\r\n",
        "\r\n",
        "# create the generator\r\n",
        "generator = define_generator(latent_dim)  # generator does not define  the loss function and the optimization method\r\n",
        "# create the gan\r\n",
        "gan_model = define_gan(generator, discriminator)\r\n",
        "# train model\r\n",
        "\r\n",
        "# Q3: The funtion \"train\" trains the gan, which involves the training of both discriminator \r\n",
        "# generator. Goto the inside of this function and answer questions.\r\n",
        " \r\n",
        "train(generator, discriminator, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=1000)\r\n",
        "# 1) train the discriminator on real samples and the fake samples generated by the current generator net\r\n",
        "# 2) Then, train the generator with the discriminator set frozen (not trainable)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}