{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuadCurveGanFinalExam.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORLf8hNwI24adkE9Qsizkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonryul/course-v3/blob/master/QuadCurveGanFinalExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LvZrYGeU8-y",
        "outputId": "9969ca02-6ee3-4925-d3d8-b41d43fabb28"
      },
      "source": [
        "!pip install tensorflow\r\n",
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.19.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqun5vTwVH2s"
      },
      "source": [
        "# train a generative adversarial network on a one-dimensional function\r\n",
        "from numpy import hstack\r\n",
        "from numpy import zeros\r\n",
        "from numpy import ones\r\n",
        "from numpy.random import rand\r\n",
        "from numpy.random import randn\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from matplotlib import pyplot\r\n",
        "\r\n",
        "\r\n",
        "# define the standalone discriminator model\r\n",
        "def define_discriminator(n_inputs=2):\r\n",
        "    # class Sequential(Model) = Linear stack of layers\r\n",
        "    # The `Model` class adds training & evaluation routines to a `Network`.\r\n",
        "    # class Model(Network): add(self, layer): \tAdds a layer instance on top of the layer stack.\r\n",
        "\r\n",
        "    model = Sequential()  # model is an object of class Sequential\r\n",
        "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\r\n",
        "    # n_inputs = 2; n_output=25:  The Input vector is a 2D point (x,y)\r\n",
        "    model.add(Dense(1, activation='sigmoid'))\r\n",
        "    # n_input = 25 = n_output of the previous layer; n_output =1 ( The value of the discriminator output is probability between  o and 1\r\n",
        "    # compile model\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "    # model.loss = loss =\"binary_crossentropy\": Using the binary cross entropy function as the loss\r\n",
        "    # function means that it tries to minimize the difference (loss) between the predicted probability of the net\r\n",
        "    # for the input to belong to one of the two categories and the labeled probability which is either 1 (real)or 0 (fake).\r\n",
        "    # It tries to minimize this difference (loss) for all inputs (whose number is 128 in our example); it means that\r\n",
        "    # it tries ot minimize the average difference (loss) of the all inputs. The basic idea is the same with the mean square error\r\n",
        "    # loss for regression problem. The difference is the \"value\" used to compute the loss probability rather than ordinary values.\r\n",
        "\r\n",
        "    return model  # model is a reference  to the current instance of the class\r\n",
        "\r\n",
        "\r\n",
        "# define the standalone generator model\r\n",
        "def define_generator(latent_dim, n_outputs=2):  # latent_dim =5\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform',\r\n",
        "                    input_dim=latent_dim))  # n_input=5 = latent_dim; n_output=15\r\n",
        "    model.add(Dense(n_outputs, activation='linear'))  # n_input = 15 = n_output of the previous layer; n_output = 2\r\n",
        "    # The dimension of the output of the geneator is 2, because it generates a 2D point (x,y) which is supposed to lie on\r\n",
        "    # the quadratic curve y = x^2;\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "# define the combined generator and discriminator model for updating the generator\r\n",
        "def define_gan(generator, discriminator):\r\n",
        "    # make weights in the discriminator not trainable\r\n",
        "    discriminator.trainable = False  # discriminator is set as not trainable when it is part of the composite model\r\n",
        "    # But it is trainable when it is used alone\r\n",
        "    # connect them\r\n",
        "    model = Sequential()\r\n",
        "    # add generator\r\n",
        "    model.add(generator)\r\n",
        "    # add the discriminator\r\n",
        "    model.add(discriminator)\r\n",
        "    # compile model\r\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\r\n",
        "    # model.loss = loss =\"binary_crossentropy\"\r\n",
        "    return model\r\n",
        "\r\n",
        "#  Making the discriminator not trainable is a clever trick in the Keras API. The trainable\r\n",
        "#  property impacts the model when it is compiled. The discriminator model was compiled with\r\n",
        "#  trainable layers, therefore the model weights in those layers will be updated when the standalone\r\n",
        "#  model is updated via calls to train on batch().\r\n",
        "\r\n",
        "# generate n real samples with class labels \"ones\" for training the discriminator\r\n",
        "def generate_real_samples(n): # n = 128/2\r\n",
        "    # generate inputs in [-0.5, 0.5]\r\n",
        "    X1 = rand(n) - 0.5\r\n",
        "    # generate outputs X^2\r\n",
        "    X2 = X1 * X1\r\n",
        "    # stack arrays\r\n",
        "    X1 = X1.reshape(n, 1)\r\n",
        "    X2 = X2.reshape(n, 1)\r\n",
        "    X = hstack((X1, X2))  # X =  hstack( [1,2], [3,4] ) ==>[ [1,3],[2,4] ] : 128 points\r\n",
        "    # generate class labels\r\n",
        "    y = ones((n, 1))  # y = 128/2 labels\r\n",
        "    return X, y  # # A pair of 128/2 real samples and their 128 labels\r\n",
        "\r\n",
        "\r\n",
        "# generate points in latent space as input for the generator\r\n",
        "def generate_latent_points(latent_dim, n):\r\n",
        "    # generate points in the latent space\r\n",
        "    x_input = randn(latent_dim * n)  # [01, 02, 0.9,...., 0,1]\r\n",
        "    # reshape into a batch of inputs for the network\r\n",
        "    x_input = x_input.reshape(n, latent_dim)  # 128 * 5 matrix\r\n",
        "    return x_input\r\n",
        "\r\n",
        "\r\n",
        "# use the generator to generate n fake examples, with class labels \"zero\", for training the discriminator\r\n",
        "def generate_fake_samples(generator, latent_dim, n): # n = 128/2\r\n",
        "    # generate points in latent space\r\n",
        "    x_input = generate_latent_points(latent_dim, n)  # 128/2  x 5: 128/2 samples of 5 random numbers\r\n",
        "    # predict outputs\r\n",
        "    X = generator.predict(x_input)  # X = 128/2 generator outputs for 128/2 samples of 5 numbers\r\n",
        "    # create class labels\r\n",
        "    y = zeros((n, 1))  # y = 128/2  labels\r\n",
        "    return X, y  # A pair of 128/2 fake samples and their 128/2 labels\r\n",
        "\r\n",
        "\r\n",
        "# evaluate the discriminator and plot real and fake points\r\n",
        "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\r\n",
        "    # prepare real samples\r\n",
        "    x_real, y_real = generate_real_samples(n)  # (x_real, y_real):  A pair of 128 real samples and their 128 labels\r\n",
        "    # evaluate discriminator on real examples\r\n",
        "    _, acc_real = discriminator.evaluate(x_real, y_real,\r\n",
        "                                         verbose=0)  # acc_real = THe accuray of the discriminator net that tells \"real\" for real samples (inputs)\r\n",
        "    # prepare fake examples\r\n",
        "    x_fake, y_fake = generate_fake_samples(generator, latent_dim,\r\n",
        "                                           n)  # (x_fake, y_fake):  # A pair of 128 fake samples and their 128 labels\r\n",
        "    # evaluate discriminator on fake examples\r\n",
        "    _, acc_fake = discriminator.evaluate(x_fake, y_fake,\r\n",
        "                                         verbose=0)  # acc_fake = The accuray of the discriminator net that tells \"fake\" for fake samples (inputs)\r\n",
        "    # summarize discriminator performance\r\n",
        "    print(epoch, acc_real, acc_fake)  # print both acc_real and acc_fake for the current epoch.\r\n",
        "    # scatter plot real and fake data points: \r\n",
        "    # x_real[i,0] is the x coord of ith real point and x_real[i,1] is the y coord of ith real point\r\n",
        "\r\n",
        "    pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\r\n",
        "    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\r\n",
        "    # save plot to file\r\n",
        "    filename = 'generated_plot_e%03d.png' % (epoch + 1)\r\n",
        "    pyplot.savefig(filename)\r\n",
        "    pyplot.close()\r\n",
        "\r\n",
        "\r\n",
        "# train the generator and discriminator\r\n",
        "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\r\n",
        "    # determine half the size of one batch, for updating the discriminator\r\n",
        "    half_batch = int(n_batch / 2)\r\n",
        "\r\n",
        "    # loop over epochs: In the case of typical supervised learning, one epoch refers to one scan over the entire dataset\r\n",
        "    # in the process of training the network.\r\n",
        "    # In this gan example, one epoch refers to one scan over a single mini-batch.\r\n",
        "    # This is because the real data is prepared not in the form of entire dataset in the beginning,\r\n",
        "    # but is computed in the form of mini-batch by \"generate_real_samples\" function at each update of the discriminator.\r\n",
        "    #\r\n",
        "    for i in range(n_epochs):\r\n",
        "        # 1) Train the discriminator to discriminate between real-data and fake-data\r\n",
        "\r\n",
        "        # 1.1)  prepare real samples (real mini-batch): note that in this example, the discriminator will see\r\n",
        "        #  10000 * 128/2 real data ( 2D points from the quadratic curve y = x^2) in total throughout all the epochs.\r\n",
        "        # It means also that the discriminator encounters the same number of fake 2D points throughout all the epochs.\r\n",
        "        # and is trained to tell \"fake\" to them. The fake data used by the discriminator become smarter as the epoch\r\n",
        "        # progresses.\r\n",
        "\r\n",
        "        x_real, y_real = generate_real_samples(half_batch)\r\n",
        "        # prepare fake examples (fake mini_batch)\r\n",
        "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\r\n",
        "        # 1.2)  update discriminator\r\n",
        "        # \"Runs a single gradient update on a single mini-batch of data\":\r\n",
        "        d_model.train_on_batch(x_real, y_real)  # train the discriminator using the real mini-batch\r\n",
        "        d_model.train_on_batch(x_fake,y_fake)  # train the discriminator using the fake mini-batch.\r\n",
        "\r\n",
        "        # 2) Train the generator net\r\n",
        "        # 2.1) prepare points in latent space as input for the generator\r\n",
        "        x_gan = generate_latent_points(latent_dim,\r\n",
        "                                       n_batch)  # x_gan = 128 x 5 matrix; This means 128 samples of 5 random numbers\r\n",
        "        # 2.2) create labels that the discriminator should produce for the fake sample output of the generator\r\n",
        "        # Note that while the generator is updated in order to \"fool\" the discriminator, the discriminator is fixed.\r\n",
        "        y_gan = ones((n_batch, 1))  # 128 1's\r\n",
        "\r\n",
        "        # 2.3) update the generator  so that the loss of the fixed discriminator is minimized.\r\n",
        "        gan_model.train_on_batch(x_gan, y_gan)   # x_gan= input to the generator = 128 samples of 5 random vectors,\r\n",
        "                                                 # y_gan = target label = 128 labels (all 1's) for the discriminator\r\n",
        "\r\n",
        "        # NOTE: gan_model.train_on_batch(x_gan, y_gan) tries to train  the generator so that the discriminator tells \"real\"\r\n",
        "        # for EVERY (fake) output generated by the generator. Note that this goal is a bit indirect in the sense that\r\n",
        "        # the generator does not attempt to optimize its output directly but it tries to optimize the\r\n",
        "        # the output of the discriminator. But it is not uncommon. For example, consider the effort of parents\r\n",
        "        # with respect to their children.\r\n",
        "\r\n",
        "        # evaluate the model every n_eval epochs\r\n",
        "        if (i + 1) % n_eval == 0:\r\n",
        "            summarize_performance(i, g_model, d_model, latent_dim)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY9MVP2yVZAd",
        "outputId": "c6cdf417-00d5-42ac-cd47-6069291d4efe"
      },
      "source": [
        "# size of the latent space (the size of a random input vector to the generator)\r\n",
        "latent_dim = 5\r\n",
        "# create the discriminator\r\n",
        "discriminator = define_discriminator()\r\n",
        "#  discriminator is a reference  to the instance of the Sequential class\r\n",
        "#  discriminator defines the loss function and the optimization method\r\n",
        "\r\n",
        "# create the generator\r\n",
        "generator = define_generator(latent_dim)  # generator does not define  the loss function and the optimization method\r\n",
        "# create the gan\r\n",
        "gan_model = define_gan(generator, discriminator)\r\n",
        "# train model\r\n",
        "train(generator, discriminator, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=1000)\r\n",
        "# 1) train the discriminator on real samples and the fake samples generated by the current generator net\r\n",
        "# 2) Then, train the generator with the discriminator set frozen (not trainable)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999 0.49000000953674316 0.44999998807907104\n",
            "1999 0.47999998927116394 0.5600000023841858\n",
            "2999 0.6399999856948853 0.4300000071525574\n",
            "3999 0.4300000071525574 0.6499999761581421\n",
            "4999 0.5400000214576721 0.5699999928474426\n",
            "5999 0.8199999928474426 0.3700000047683716\n",
            "6999 0.7699999809265137 0.5199999809265137\n",
            "7999 0.7300000190734863 0.3499999940395355\n",
            "8999 0.6100000143051147 0.47999998927116394\n",
            "9999 0.7099999785423279 0.4399999976158142\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}